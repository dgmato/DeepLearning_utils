{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Deep Learning Example Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, model_from_json\n",
    "from keras import layers\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Dropout, Activation, Flatten, Dense, AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.metrics import binary_crossentropy, mean_absolute_error, categorical_crossentropy, mean_squared_error, hinge\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, CSVLogger, BaseLogger, Callback, EarlyStopping, TensorBoard, LambdaCallback, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, auc, confusion_matrix, roc_curve, fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_BiometryNet():\n",
    "    \n",
    "    ## Conv Model\n",
    "    map_input = Input((im_w, im_h, 2))\n",
    "    \n",
    "    filters_init = 16\n",
    "    kernel_size = (3,3)\n",
    "    batch_norm = True\n",
    "    dropout = False\n",
    "    \n",
    "    block1 = conv_conv_MaxPool_block(map_input, filters_init, kernel_size, batch_norm, dropout, 2)\n",
    "    \n",
    "    block2 = conv_conv_MaxPool_block(block1, filters_init*2, kernel_size, batch_norm, dropout, 2)\n",
    "    \n",
    "    block3 = conv_conv_MaxPool_block(block2, filters_init*4, kernel_size, batch_norm, dropout, 2)\n",
    "    \n",
    "    block4 = conv_conv_MaxPool_block(block3, filters_init*8, kernel_size, batch_norm, dropout, 2)\n",
    "    \n",
    "    flatten = Flatten()(block4)\n",
    "    \n",
    "    model_convA = Model(inputs=map_input, outputs=[flatten])\n",
    "    model_convB = Model(inputs=map_input, outputs=[flatten])\n",
    "    ## FC Model\n",
    "    map_inputA = Input((im_w, im_h, 2))\n",
    "    map_inputB = Input((im_w, im_h, 2))\n",
    "    \n",
    "    flattenA = model_convA(map_inputA)\n",
    "    flattenB = model_convB(map_inputB)\n",
    "    \n",
    "    dense1 = concatenate([flattenA, flattenB], axis = 1)\n",
    "    \n",
    "    dense1 = Dense(2048, activation=\"relu\")(dense1)\n",
    "    \n",
    "    dense2 = Dense(2048, activation=\"relu\")(dense1)\n",
    "\n",
    "    output = Dense(2, activation='softmax', kernel_initializer=\"glorot_normal\")(dense2)\n",
    "    \n",
    "    model = Model(inputs=[map_inputA, map_inputB], outputs=[output])\n",
    "    \n",
    "\n",
    "    model.compile(optimizer=Adam(lr=0.00001), loss=categorical_crossentropy, metrics=['categorical_accuracy'])\n",
    "    \n",
    "    print('Biometry-net_5 Initialized')\n",
    "\n",
    "    return model, model_convA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "#model, model_conv = get_BiometryNet()\n",
    "path_results = '/home/rmoreta/Projects/Biometria/Results/general/'\n",
    "plot_model(model, to_file=path_results+'model_BiometryNet5.png', show_shapes=True, show_layer_names=True)\n",
    "plot_model(model_conv, to_file=path_results+'model_BiometryNet5_model_conv.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model, model_conv = get_BiometryNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** VGG **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_block(x, filters, kernel_size, bn, num_conv, block):\n",
    "    \"\"\"\n",
    "    # Arguments\n",
    "    \n",
    "    # Returns\n",
    "    \n",
    "    \"\"\"    \n",
    "    conv_name = 'block' + str(block) + '_conv'\n",
    "    bn_name = 'block' + str(block) + '_bn'\n",
    "    pool_name = 'block' + str(block) + '_pool'\n",
    "    activ_name = 'block' + str(block) + '_activation'\n",
    "    \n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    \n",
    "    \n",
    "    for conv_i in range(num_conv):\n",
    "        x = Conv2D(filters, kernel_size, name=conv_name + str(conv_i+1))(x)\n",
    "        x = BatchNormalization(axis=bn_axis, name=bn_name + str(conv_i+1))(x) if bn else x\n",
    "        x = Activation('relu', name=activ_name + str(conv_i+1))(x)\n",
    "        \n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name=pool_name)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vgg16(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    # Arguments\n",
    "    \n",
    "    # Returns\n",
    "    \n",
    "    \"\"\"\n",
    "    kernel_size = (3,3)\n",
    "    filters_init = 32\n",
    "    bn = 1\n",
    "    \n",
    "    inputs = Input(shape=input_shape, name='input')\n",
    "    \n",
    "    block1 = conv_block(inputs, filters_init, kernel_size, bn, num_conv=2, block=1)\n",
    "    \n",
    "    block2 = conv_block(block1, filters_init*2, kernel_size, bn, num_conv=2, block=2)\n",
    "    \n",
    "    block3 = conv_block(block2, filters_init*4, kernel_size, bn, num_conv=3, block=3)\n",
    "    \n",
    "    block4 = conv_block(block3, filters_init*8, kernel_size, bn, num_conv=3, block=4)\n",
    "    \n",
    "    block5 = conv_block(block4, filters_init*16, kernel_size, bn, num_conv=3, block=5)\n",
    "    \n",
    "    flatten = Flatten(name='flatten')(block5)\n",
    "    \n",
    "    fc1 = Dense(2048, activation='relu', name='fc1')(flatten)\n",
    "    \n",
    "    fc2 = Dense(2048, activation='relu', name='fc2')(fc1)\n",
    "    \n",
    "    predictions = Dense(num_classes, activation='softmax', name='predictions')(fc2)\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[predictions], name='vgg16')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 256, 256, 2)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 254, 254, 32)      608       \n",
      "_________________________________________________________________\n",
      "block1_bn1 (BatchNormalizati (None, 254, 254, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 254, 254, 32)      0         \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 252, 252, 32)      9248      \n",
      "_________________________________________________________________\n",
      "block1_bn2 (BatchNormalizati (None, 252, 252, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 252, 252, 32)      0         \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 126, 126, 32)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 124, 124, 64)      18496     \n",
      "_________________________________________________________________\n",
      "block2_bn1 (BatchNormalizati (None, 124, 124, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 124, 124, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 122, 122, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block2_bn2 (BatchNormalizati (None, 122, 122, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 122, 122, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 61, 61, 64)        0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 59, 59, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block3_bn1 (BatchNormalizati (None, 59, 59, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 59, 59, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 57, 57, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block3_bn2 (BatchNormalizati (None, 57, 57, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 57, 57, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 55, 55, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block3_bn3 (BatchNormalizati (None, 55, 55, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 55, 55, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 27, 27, 128)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 25, 25, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block4_bn1 (BatchNormalizati (None, 25, 25, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 25, 25, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 23, 23, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block4_bn2 (BatchNormalizati (None, 23, 23, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 23, 23, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 21, 21, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block4_bn3 (BatchNormalizati (None, 21, 21, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 21, 21, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block5_bn1 (BatchNormalizati (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_bn2 (BatchNormalizati (None, 6, 6, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_bn3 (BatchNormalizati (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 16,217,730\n",
      "Trainable params: 16,211,970\n",
      "Non-trainable params: 5,760\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = vgg16((256,256,2), 2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** VGG16-Biometry **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_block(x, filters, kernel_size, bn, num_conv, block):\n",
    "    \"\"\"\n",
    "    # Arguments\n",
    "    \n",
    "    # Returns\n",
    "    \n",
    "    \"\"\"    \n",
    "    conv_name = 'block' + str(block) + '_conv'\n",
    "    bn_name = 'block' + str(block) + '_bn'\n",
    "    pool_name = 'block' + str(block) + '_pool'\n",
    "    activ_name = 'block' + str(block) + '_activation'\n",
    "    \n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    \n",
    "    \n",
    "    for conv_i in range(num_conv):\n",
    "        x = Conv2D(filters, kernel_size, name=conv_name + str(conv_i+1))(x)\n",
    "        x = BatchNormalization(axis=bn_axis, name=bn_name + str(conv_i+1))(x) if bn else x\n",
    "        x = Activation('relu', name=activ_name + str(conv_i+1))(x)\n",
    "        \n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name=pool_name)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg16_biometry(input_shape, num_classes, option):\n",
    "    \"\"\"\n",
    "    # Arguments\n",
    "    \n",
    "    # Returns\n",
    "    \n",
    "    \"\"\"\n",
    "    kernel_size = (3,3)\n",
    "    filters_init = 32\n",
    "    bn = 1\n",
    "    \n",
    "    inputs = Input(shape=input_shape, name='inputs_conv')\n",
    "    \n",
    "    block1 = conv_block(inputs, filters_init, kernel_size, bn, num_conv=2, block=1)\n",
    "    \n",
    "    block2 = conv_block(block1, filters_init*2, kernel_size, bn, num_conv=2, block=2)\n",
    "    \n",
    "    block3 = conv_block(block2, filters_init*4, kernel_size, bn, num_conv=3, block=3)\n",
    "    \n",
    "    block4 = conv_block(block3, filters_init*8, kernel_size, bn, num_conv=3, block=4)\n",
    "    \n",
    "    block5 = conv_block(block4, filters_init*16, kernel_size, bn, num_conv=3, block=5)\n",
    "    \n",
    "    flatten = Flatten(name='flatten')(block5)\n",
    "    \n",
    "    model_conv = Model(inputs=inputs, outputs=[flatten])\n",
    "    \n",
    "    \n",
    "    map_inputA = Input((im_w, im_h, 2))\n",
    "    map_inputB = Input((im_w, im_h, 2))\n",
    "    \n",
    "    \n",
    "    flattenA = model_conv(map_inputA)\n",
    "    if option == 1: flattenB = model_conv(map_inputB)\n",
    "    elif option == 2: flattenB = model_convB(map_inputB)\n",
    "    \n",
    "    conc = concatenate([flattenA, flattenB], axis = 1)\n",
    "    \n",
    "    fc1 = Dense(2048, activation='relu', name='fc1')(conc)\n",
    "    \n",
    "    fc2 = Dense(2048, activation='relu', name='fc2')(fc1)\n",
    "    \n",
    "    predictions = Dense(num_classes, activation='softmax', name='predictions')(fc2)\n",
    "    \n",
    "    model = Model(inputs=[map_inputA, map_inputB], outputs=[predictions], name='vgg16_biometry')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AlexNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** GoogLeNet **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** U-Net **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unet():\n",
    "    \n",
    "    inputs = Input((512, 512, 1))\n",
    "    \n",
    "    conv1 = Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"glorot_normal\", activation=\"relu\")(inputs)\n",
    "    conv1 = Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"glorot_normal\", activation=\"relu\")(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"glorot_normal\", activation=\"relu\")(pool1)\n",
    "    conv2 = Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"glorot_normal\", activation=\"relu\")(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"glorot_normal\", activation=\"relu\")(pool2)\n",
    "    conv3 = Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"glorot_normal\", activation=\"relu\")(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"glorot_normal\", activation=\"relu\")(pool3)\n",
    "    conv4 = Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"glorot_normal\", activation=\"relu\")(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    \n",
    "    conv5 = Conv2D(512, (3, 3), padding=\"same\", kernel_initializer=\"glorot_normal\", activation=\"relu\")(pool4)\n",
    "    conv5 = Conv2D(512, (3, 3), padding=\"same\", kernel_initializer=\"glorot_normal\", activation=\"relu\")(conv5)\n",
    "    \n",
    "    \n",
    "    up6 = concatenate([Conv2D(256, (2, 2),  padding=\"same\", kernel_initializer=\"glorot_normal\", activation=\"relu\")(UpSampling2D(size=(2, 2))(conv5)), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"glorot_normal\", activation=\"relu\")(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"glorot_normal\", activation=\"relu\")(conv6)\n",
    "\n",
    "    up7 = concatenate([Conv2D(128, (2, 2),  padding=\"same\", kernel_initializer=\"glorot_normal\", activation=\"relu\")(UpSampling2D(size=(2, 2))(conv6)), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"glorot_normal\", activation=\"relu\")(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"glorot_normal\", activation=\"relu\")(conv7)\n",
    "\n",
    "    up8 = concatenate([Conv2D(64,(2, 2),  padding=\"same\", kernel_initializer=\"glorot_normal\", activation=\"relu\")(UpSampling2D(size=(2, 2))(conv7)), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"glorot_normal\", activation=\"relu\")(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"glorot_normal\", activation=\"relu\")(conv8)\n",
    "\n",
    "    up9 = concatenate([Conv2D(32,(2, 2),  padding=\"same\", kernel_initializer=\"glorot_normal\", activation=\"relu\")(UpSampling2D(size=(2, 2))(conv8)), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"glorot_normal\", activation=\"relu\")(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"glorot_normal\", activation=\"relu\")(conv9)\n",
    "\n",
    "    conv10 = Conv2D(num_classes, (1, 1), activation='sigmoid', kernel_initializer=\"glorot_normal\")(conv9)\n",
    "    \n",
    "    \n",
    "    model = Model(outputs=[conv10], inputs=inputs)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=0.0001),loss=categorical_crossentropy, metrics=['accuracy'])\n",
    "    \n",
    "    print('U-net Initialized')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unet_downblock(x, filters, kernel_size, bn, block):\n",
    "    \"\"\"\n",
    "    # Arguments\n",
    "    \n",
    "    # Returns\n",
    "    \n",
    "    \"\"\"\n",
    "    conv_name = 'block' + str(block) + '_conv'\n",
    "    bn_name = 'block' + str(block) + '_bn'\n",
    "    pool_name = 'block' + str(block) + '_bn'\n",
    "    activ_name = 'block' + str(block) + '_activation'\n",
    "    \n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    \n",
    "    \n",
    "    x = Conv2D(filters, kernel_size, padding=\"same\", name=conv_name+'1')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name+'1')(x) if bn else x\n",
    "    x = Activation('relu', name=activ_name+'1')(x)\n",
    "    \n",
    "    x = Conv2D(filters, kernel_size, padding=\"same\", name=conv_name+'2')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name+'2')(x) if bn else x\n",
    "    x = Activation('relu', name=activ_name+'2')(x)\n",
    "    \n",
    "    pool = MaxPooling2D(pool_size=(2, 2), name=pool_name)(x)\n",
    "    \n",
    "    return pool, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unet_upblock(x, conv_down, filters, kernel_size, bn, block):\n",
    "    \"\"\"\n",
    "    # Arguments\n",
    "    \n",
    "    # Returns\n",
    "    \n",
    "    \"\"\"\n",
    "    conv_name = 'block' + str(block) + '_conv'\n",
    "    bn_name = 'block' + str(block) + '_bn'\n",
    "    pool_name = 'block' + str(block) + '_bn'\n",
    "    activ_name = 'block' + str(block) + '_activation'\n",
    "    \n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "        \n",
    "    \n",
    "    up = UpSampling2D(size=(2, 2))(x)\n",
    "    conv_up = Conv2D(filters, (2, 2),  padding=\"same\", activation=\"relu\", name=conv_name + 'Up')(up)\n",
    "    concat = concatenate([conv_up, conv_down], axis=3, name='block'+str(block)+'_conc')\n",
    "    \n",
    "    x = Conv2D(filters, kernel_size, padding=\"same\", name=conv_name+'1')(concat)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name+'1')(x) if bn else x\n",
    "    x = Activation('relu', name=activ_name+'1')(x)\n",
    "    \n",
    "    x = Conv2D(filters, kernel_size, padding=\"same\", name=conv_name + '2')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name+'2')(x) if bn else x\n",
    "    x = Activation('relu', name=activ_name+'2')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unet(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    # Arguments\n",
    "    \n",
    "    # Returns\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    kernel_size = (3,3)\n",
    "    filters_init = 32\n",
    "    bn = 0\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    \n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    poolBlock1, convBlock1 = unet_downblock(inputs, filters_init, kernel_size, bn, block=1)\n",
    "    \n",
    "    poolBlock2, convBlock2 = unet_downblock(poolBlock1, filters_init*2, kernel_size, bn, block=2)\n",
    "    \n",
    "    poolBlock3, convBlock3 = unet_downblock(poolBlock2, filters_init*4, kernel_size, bn, block=3)\n",
    "    \n",
    "    poolBlock4, convBlock4 = unet_downblock(poolBlock3, filters_init*8, kernel_size, bn, block=4)\n",
    "    \n",
    "    \n",
    "    convBlock5 = Conv2D(filters_init*16, (3, 3), padding=\"same\", name='block5_conv1')(poolBlock4)\n",
    "    convBlock5 = BatchNormalization(axis=bn_axis, name='block5_bn1')(convBlock5) if bn else convBlock5\n",
    "    convBlock5 = Activation('relu', name='block5_activation1')(convBlock5)\n",
    "    convBlock5 = Conv2D(filters_init*16, (3, 3), padding=\"same\", name='block5_conv2')(convBlock5)\n",
    "    convBlock5 = BatchNormalization(axis=bn_axis, name='block5_bn2')(convBlock5) if bn else convBlock5\n",
    "    convBlock5 = Activation('relu', name='block5_activation2')(convBlock5)\n",
    "    \n",
    "    \n",
    "    convBlock6 = unet_upblock(convBlock5, convBlock4, filters_init*8, kernel_size, bn, block=6)\n",
    "    \n",
    "    convBlock7 = unet_upblock(convBlock6, convBlock3, filters_init*4, kernel_size, bn, block=7)\n",
    "    \n",
    "    convBlock8 = unet_upblock(convBlock7, convBlock2, filters_init*2, kernel_size, bn, block=8)\n",
    "    \n",
    "    convBlock9 = unet_upblock(convBlock8, convBlock1, filters_init, kernel_size, bn, block=9)\n",
    "    \n",
    "    \n",
    "    convBlock10 = Conv2D(num_classes, (1, 1), activation='sigmoid', name='block10_conv')(convBlock9)\n",
    "    \n",
    "    model = Model(outputs=[convBlock10], inputs=inputs, name='u-net')\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_13 (InputLayer)            (None, 512, 512, 1)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)            (None, 512, 512, 32)  320         input_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "block1_activation1 (Activation)  (None, 512, 512, 32)  0           block1_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)            (None, 512, 512, 32)  9248        block1_activation1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block1_activation2 (Activation)  (None, 512, 512, 32)  0           block1_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block1_bn (MaxPooling2D)         (None, 256, 256, 32)  0           block1_activation2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)            (None, 256, 256, 64)  18496       block1_bn[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "block2_activation1 (Activation)  (None, 256, 256, 64)  0           block2_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)            (None, 256, 256, 64)  36928       block2_activation1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block2_activation2 (Activation)  (None, 256, 256, 64)  0           block2_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_bn (MaxPooling2D)         (None, 128, 128, 64)  0           block2_activation2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)            (None, 128, 128, 128) 73856       block2_bn[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "block3_activation1 (Activation)  (None, 128, 128, 128) 0           block3_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)            (None, 128, 128, 128) 147584      block3_activation1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block3_activation2 (Activation)  (None, 128, 128, 128) 0           block3_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_bn (MaxPooling2D)         (None, 64, 64, 128)   0           block3_activation2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)            (None, 64, 64, 256)   295168      block3_bn[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "block4_activation1 (Activation)  (None, 64, 64, 256)   0           block4_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)            (None, 64, 64, 256)   590080      block4_activation1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block4_activation2 (Activation)  (None, 64, 64, 256)   0           block4_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_bn (MaxPooling2D)         (None, 32, 32, 256)   0           block4_activation2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)            (None, 32, 32, 512)   1180160     block4_bn[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "block5_activation1 (Activation)  (None, 32, 32, 512)   0           block5_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)            (None, 32, 32, 512)   2359808     block5_activation1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block5_activation2 (Activation)  (None, 32, 32, 512)   0           block5_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_24 (UpSampling2D)  (None, 64, 64, 512)   0           block5_activation2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block6_convUp (Conv2D)           (None, 64, 64, 256)   524544      up_sampling2d_24[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "block6_conc (Concatenate)        (None, 64, 64, 512)   0           block6_convUp[0][0]              \n",
      "                                                                   block4_activation2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block6_conv1 (Conv2D)            (None, 64, 64, 256)   1179904     block6_conc[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block6_activation1 (Activation)  (None, 64, 64, 256)   0           block6_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block6_conv2 (Conv2D)            (None, 64, 64, 256)   590080      block6_activation1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block6_activation2 (Activation)  (None, 64, 64, 256)   0           block6_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_25 (UpSampling2D)  (None, 128, 128, 256) 0           block6_activation2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block7_convUp (Conv2D)           (None, 128, 128, 128) 131200      up_sampling2d_25[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "block7_conc (Concatenate)        (None, 128, 128, 256) 0           block7_convUp[0][0]              \n",
      "                                                                   block3_activation2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block7_conv1 (Conv2D)            (None, 128, 128, 128) 295040      block7_conc[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block7_activation1 (Activation)  (None, 128, 128, 128) 0           block7_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block7_conv2 (Conv2D)            (None, 128, 128, 128) 147584      block7_activation1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block7_activation2 (Activation)  (None, 128, 128, 128) 0           block7_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_26 (UpSampling2D)  (None, 256, 256, 128) 0           block7_activation2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block8_convUp (Conv2D)           (None, 256, 256, 64)  32832       up_sampling2d_26[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "block8_conc (Concatenate)        (None, 256, 256, 128) 0           block8_convUp[0][0]              \n",
      "                                                                   block2_activation2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block8_conv1 (Conv2D)            (None, 256, 256, 64)  73792       block8_conc[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block8_activation1 (Activation)  (None, 256, 256, 64)  0           block8_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block8_conv2 (Conv2D)            (None, 256, 256, 64)  36928       block8_activation1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block8_activation2 (Activation)  (None, 256, 256, 64)  0           block8_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_27 (UpSampling2D)  (None, 512, 512, 64)  0           block8_activation2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block9_convUp (Conv2D)           (None, 512, 512, 32)  8224        up_sampling2d_27[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "block9_conc (Concatenate)        (None, 512, 512, 64)  0           block9_convUp[0][0]              \n",
      "                                                                   block1_activation2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block9_conv1 (Conv2D)            (None, 512, 512, 32)  18464       block9_conc[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block9_activation1 (Activation)  (None, 512, 512, 32)  0           block9_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block9_conv2 (Conv2D)            (None, 512, 512, 32)  9248        block9_activation1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block9_activation2 (Activation)  (None, 512, 512, 32)  0           block9_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block10_conv (Conv2D)            (None, 512, 512, 2)   66          block9_activation2[0][0]         \n",
      "====================================================================================================\n",
      "Total params: 7,759,554\n",
      "Trainable params: 7,759,554\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = unet((512,512,1), 2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ResNet **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Conv2D(filters1, (1, 1), name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size,\n",
    "               padding='same', name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    x = layers.add([x, input_tensor])\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n",
    "    \"\"\"A block that has a conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    Note that from stage 3, the first conv layer at main path is with strides=(2,2)\n",
    "    And the shortcut should have strides=(2,2) as well\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Conv2D(filters1, (1, 1), strides=strides,\n",
    "               name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size, padding='same',\n",
    "               name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    shortcut = Conv2D(filters3, (1, 1), strides=strides,\n",
    "                      name=conv_name_base + '1')(input_tensor)\n",
    "    shortcut = BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut)\n",
    "\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ResNet50(input_shape, classes):\n",
    "\n",
    "    img_input = Input(shape=input_shape)\n",
    "            \n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "\n",
    "    x = Conv2D(\n",
    "        64, (7, 7), strides=(2, 2), padding='same', name='conv1')(img_input)\n",
    "    x = BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n",
    "\n",
    "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n",
    "\n",
    "    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
    "\n",
    "    x = AveragePooling2D((3, 3), name='avg_pool')(x)\n",
    "\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(classes, activation='softmax', name='fc')(x)\n",
    "\n",
    "    inputs = img_input\n",
    "    \n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='resnet50')\n",
    "    \n",
    "    print 'ResNet50 Initialized'\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_weight = {0: , 1:}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_net = '/home/rmoreta/Projects/Biometria/Results/nets/'\n",
    "name_net = 'Biometria_Prueba5_12epochs.hdf5'\n",
    "model.load_weights(path_net + name_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.train_losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.train_acc.append(logs.get('categorical_accuracy'))\n",
    "        self.val_acc.append(logs.get('val_categorical_accuracy'))\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        pass\n",
    "history = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save model in .hdf5\n",
    "path_unet = '/home/rmoreta/Projects/PectoralisSegmentation/Results/'\n",
    "name_unet = 'unet_nc' + str(num_classes) + '_pecs_' + str(num_images_train) + 'prueba_k2' + '.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(path_unet + name_unet, monitor='loss',verbose=1, save_best_only=True)\n",
    "\n",
    "# Create a CVS with the values of loss and accuracy at each epoch\n",
    "model_CVSLogger = CSVLogger('hola.cvs', separator=',', append=True)\n",
    "\n",
    "# Gets loss values at each batch and epoch in a list\n",
    "history = LossHistory()\n",
    "\n",
    "# Early Stopping \n",
    "model_EarlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "# Reduce LR\n",
    "model_ReduceLROnPlateau = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=2, verbose=1, mode='auto', \n",
    "                  cooldown=0, min_lr=0.00005) # epsilon=0.0001\n",
    "\n",
    "# Tensorboard\n",
    "model_TensorBoard = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit, test, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "nb_epoch = 5\n",
    "\n",
    "\n",
    "# train_mapasA = train_mapas[:,:,:,0].reshape([num_train_mapas,141,141,1])\n",
    "# train_mapasB = train_mapas[:,:,:,1].reshape([num_train_mapas,141,141,1])\n",
    "\n",
    "# model.fit(train_mapas, categorical_labels, batch_size=batch_size, epochs=nb_epoch, \n",
    "#           verbose=1, shuffle=True)\n",
    "\n",
    "model.fit([train_mapas, train_mapas2], train_cat_labels, \n",
    "          validation_data = ([val_mapas, val_mapas2], val_cat_labels),\n",
    "          batch_size=batch_size, epochs=nb_epoch, \n",
    "          verbose=1, shuffle=True,\n",
    "         callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_on_batch, test_on_batch, predict_on_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "for epoch_i in range(num_epochs):\n",
    "    batch_size = \n",
    "    num_batches = num_train_sampels // batch_size\n",
    "    \n",
    "    for batch_i in range(num_batches):\n",
    "        inputA =\n",
    "        inputB = \n",
    "        cat_labels = \n",
    "        \n",
    "        model.train_on_batch([inputA, inputB], cat_labels)\n",
    "        \n",
    "        model.test_on_batch([test_mapas, test_mapas2], test_cat_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "# batch_size = 16\n",
    "# seed = 9\n",
    "loss = list()\n",
    "loss_epoch = list()\n",
    "\n",
    "for epoch_i in range(num_epochs):\n",
    "    print 'Epoch', epoch_i+1\n",
    "    num_batches = len(train_index_list_random_batch)\n",
    "    \n",
    "    for batch_i in range(num_batches):\n",
    "        print 'Batch:', batch_i, '/', num_batches\n",
    "        inputA, train_labels, n = data_preparation(curv_back_OD, subject_matrix, registros_matrix, train_index_list_random_batch[batch_i])\n",
    "        inputB, train_labels2, n = data_preparation(curv_front_OD, subject_matrix, registros_matrix, train_index_list_random_batch[batch_i])\n",
    "        cat_labels =  to_categorical(train_labels, num_classes=2)\n",
    "        \n",
    "        aux_loss = model.train_on_batch([inputA, inputB], cat_labels)\n",
    "        \n",
    "        loss.append(aux_loss)\n",
    "        \n",
    "        print 'Batch:', batch_i, '/', num_batches, ', loss:', aux_loss\n",
    "    loss_epoch.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_loss = list()\n",
    "test_acc = list()\n",
    "num_batches = len(test_index_list_random_batch)\n",
    "for batch_i in range(num_batches):\n",
    "    print 'Batch:', batch_i+1, '/', num_batches\n",
    "    inputA, test_labels, n = data_preparation(curv_back_OD, subject_matrix, registros_matrix, test_index_list_random_batch[batch_i])\n",
    "    inputB, test_labels2, n = data_preparation(curv_front_OD, subject_matrix, registros_matrix, test_index_list_random_batch[batch_i])\n",
    "    test_cat_labels =  to_categorical(test_labels, num_classes=2)\n",
    "    \n",
    "    aux_loss, aux_acc = model.test_on_batch([inputA, inputB], test_cat_labels)\n",
    "    \n",
    "    test_loss.append(aux_loss)\n",
    "    test_acc.append(aux_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit_generator, evaluate_generator, predict_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random_subj_indexes_from_matrix(subject_matrix, subj_list, seed, train):\n",
    "    if train == 1:\n",
    "        aux_index = get_train_index(subject_matrix, subj_list)\n",
    "    else:\n",
    "        aux_index = get_test_index(subject_matrix, subj_list)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    aux_index_pos = np.where(aux_index)[0]\n",
    "    np.random.shuffle(aux_index_pos)\n",
    "    \n",
    "    return aux_index_pos\n",
    "\n",
    "def get_batch_indexes_balanced_labels(labels_matrix, index_list, batch_size):\n",
    "\n",
    "    data_len = len(index_list)\n",
    "\n",
    "    num_labels_1 = sum(labels_matrix[index_list])\n",
    "    num_labels_0 = data_len - num_labels_1\n",
    "    print 'data_len', data_len\n",
    "    print 'num_labels_1', num_labels_1\n",
    "    \n",
    "    num_batches = int(np.ceil(float(data_len)/batch_size))\n",
    "    print 'num_batches:', num_batches\n",
    "\n",
    "    aux_index_1 = index_list[np.where(labels_matrix[index_list] == 1)]\n",
    "    aux_index_0 = index_list[np.where(labels_matrix[index_list] == 0)]\n",
    "\n",
    "    num_1_per_batch = float(num_labels_1)/data_len*batch_size\n",
    "#     num_0_per_batch = batch_size - num_1_per_batch\n",
    "\n",
    "    numero = num_labels_1 - num_batches*int(num_1_per_batch)\n",
    "    if numero<0: numero = 0\n",
    "    print 'numero', numero\n",
    "    print 'num_1_per_batch', num_1_per_batch\n",
    "\n",
    "    index_list_batch = []\n",
    "    aux_0 = 0\n",
    "    aux_1 = 0\n",
    "    for i in range(num_batches):\n",
    "\n",
    "#         if i == 0 and numero == 0:\n",
    "#             num_1_per_batch = int(np.ceil(num_1_per_batch)) - 1\n",
    "#             num_0_per_batch = batch_size - num_1_per_batch + 1\n",
    "            \n",
    "        if i == 0:\n",
    "            num_1_per_batch = int(np.ceil(num_1_per_batch))\n",
    "            num_0_per_batch = batch_size - num_1_per_batch\n",
    "            print num_1_per_batch\n",
    "\n",
    "        if i == numero:\n",
    "            num_1_per_batch -= 1\n",
    "            num_0_per_batch += 1\n",
    "\n",
    "        aux_index_batch = aux_index_0[aux_0:aux_0+num_0_per_batch]\n",
    "        aux_index_batch = np.concatenate((aux_index_batch, aux_index_1[aux_1:aux_1+num_1_per_batch]))\n",
    "        aux_0 += num_0_per_batch\n",
    "        aux_1 += num_1_per_batch\n",
    "\n",
    "        np.random.shuffle(aux_index_batch)\n",
    "\n",
    "        index_list_batch.extend(aux_index_batch)\n",
    "    \n",
    "    index_list_batch = np.array(index_list_batch)\n",
    "#     np.random.shuffle(index_list_batch)\n",
    "    return index_list_batch\n",
    "\n",
    "\n",
    "def data_preparation(mapas_dataset, subject_matrix, registros_matrix, index_list):\n",
    "    data_mapas, data_labels = [], []\n",
    "    im_w = mapas_dataset[0][0].shape[0]\n",
    "    im_h = mapas_dataset[0][0].shape[1]\n",
    "    \n",
    "    for i in range(len(index_list)):\n",
    "        subj_i = subject_matrix[index_list[i], 0]\n",
    "        subj_j = subject_matrix[index_list[i], 1]\n",
    "\n",
    "        reg_i = registros_matrix[index_list[i], 0]\n",
    "        reg_j = registros_matrix[index_list[i], 1]\n",
    "\n",
    "        if subj_i == subj_j:\n",
    "            aux_label = 1\n",
    "        else:\n",
    "            aux_label = 0\n",
    "\n",
    "        aux_mapas = np.zeros([im_w, im_h, 2])\n",
    "        aux_mapas[:,:,0] = mapas_dataset[subj_i][reg_i]\n",
    "        aux_mapas[:,:,1] = mapas_dataset[subj_j][reg_j]\n",
    "\n",
    "        data_mapas.append(aux_mapas)\n",
    "        data_labels.append(aux_label)\n",
    "\n",
    "    data_mapas = np.array(data_mapas)\n",
    "    data_labels = np.array(data_labels, dtype='int8')\n",
    "    num_data_mapas = data_mapas.shape[0]\n",
    "\n",
    "    return data_mapas, data_labels, num_data_mapas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "class DataGenerator(object):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, dim_x = 141, dim_y = 141, num_channels = 2, batch_size = 32):\n",
    "        'Initialization'\n",
    "        self.dim_x = dim_x\n",
    "        self.dim_y = dim_y\n",
    "        self.num_channels = num_channels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate(self, mapas_dataset, subject_matrix, registros_matrix, labels_matrix, subj_list, seed, param_train):\n",
    "        'Generates batches of samples'\n",
    "        # Infinite loop\n",
    "        while 1:\n",
    "            # Generate order of exploration of dataset  \n",
    "            index_list_random = self.__get_random_subj_indexes_from_matrix(subject_matrix, subj_list, seed, param_train)\n",
    "            index_list_random_batch = self.__get_batch_indexes_balanced_labels(labels_matrix, index_list_random, self.batch_size)\n",
    "\n",
    "            # Generate batches\n",
    "            num_batches = int(np.ceil(len(index_list_random)/float(self.batch_size)))\n",
    "            for i in range(num_batches):\n",
    "                # Find list of indexes for each batch\n",
    "                index_list_temp = index_list_random_batch[i*self.batch_size:(i+1)*self.batch_size]\n",
    "\n",
    "                # Generate data\n",
    "                X, y = self.__data_generation(mapas_dataset, subject_matrix, registros_matrix, labels_matrix, index_list_temp)\n",
    "\n",
    "                yield X, y\n",
    "\n",
    "\n",
    "    def __get_random_subj_indexes_from_matrix(subject_matrix, subj_list, seed, param_train):\n",
    "        if param_train == 1:\n",
    "            aux_index = self.get_train_index(subject_matrix, subj_list)\n",
    "        elif param_train == 0:\n",
    "            aux_index = self.get_test_index(subject_matrix, subj_list)\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        aux_index_pos = np.where(aux_index)[0]\n",
    "        np.random.shuffle(aux_index_pos)\n",
    "\n",
    "        return aux_index_pos\n",
    "    \n",
    "    def __get_batch_indexes_balanced_labels(labels_matrix, index_list, batch_size):\n",
    "\n",
    "        data_len = len(index_list)\n",
    "\n",
    "        num_labels_1 = sum(labels_matrix[index_list])\n",
    "        num_labels_0 = data_len - num_labels_1\n",
    "\n",
    "        num_batches = int(np.ceil(float(data_len)/batch_size))\n",
    "\n",
    "        aux_index_1 = index_list[np.where(labels_matrix[index_list] == 1)]\n",
    "        aux_index_0 = index_list[np.where(labels_matrix[index_list] == 0)]\n",
    "\n",
    "        num_1_per_batch = float(num_labels_1)/data_len*batch_size\n",
    "\n",
    "        numero = num_labels_1 - num_batches*int(num_1_per_batch)\n",
    "        if numero<0: numero = 0\n",
    "\n",
    "        index_list_batch = []\n",
    "        aux_0 = 0\n",
    "        aux_1 = 0\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "\n",
    "            if i == 0:\n",
    "                num_1_per_batch = int(np.ceil(num_1_per_batch))\n",
    "                num_0_per_batch = batch_size - num_1_per_batch\n",
    "                print num_1_per_batch\n",
    "\n",
    "            if i == numero:\n",
    "                num_1_per_batch -= 1\n",
    "                num_0_per_batch += 1\n",
    "\n",
    "            aux_index_batch = aux_index_0[aux_0:aux_0+num_0_per_batch]\n",
    "            aux_index_batch = np.concatenate((aux_index_batch, aux_index_1[aux_1:aux_1+num_1_per_batch]))\n",
    "            aux_0 += num_0_per_batch\n",
    "            aux_1 += num_1_per_batch\n",
    "\n",
    "            np.random.shuffle(aux_index_batch)\n",
    "\n",
    "            index_list_batch.extend(aux_index_batch)\n",
    "\n",
    "        index_list_batch = np.array(index_list_batch)\n",
    "        \n",
    "        return index_list_batch\n",
    "\n",
    "    def __data_generation(mapas_dataset, subject_matrix, registros_matrix, labels_matrix, index_list):\n",
    "        'Generates data of batch_size samples' \n",
    "        # Initialization         \n",
    "        data_mapas, data_labels = [], []\n",
    "\n",
    "        # Get mapas\n",
    "        for i in range(len(index_list)):\n",
    "            subj_i = subject_matrix[index_list[i], 0]\n",
    "            subj_j = subject_matrix[index_list[i], 1]\n",
    "\n",
    "            reg_i = registros_matrix[index_list[i], 0]\n",
    "            reg_j = registros_matrix[index_list[i], 1]\n",
    "\n",
    "            aux_mapas = np.zeros([self.dim_x, self.dim_y, self.num_channels])\n",
    "            aux_mapas[:,:,0] = mapas_dataset[subj_i][reg_i]\n",
    "            aux_mapas[:,:,1] = mapas_dataset[subj_j][reg_j]\n",
    "\n",
    "            aux_label = labels_matrix[index_list[i]]\n",
    "            \n",
    "            data_mapas.append(aux_mapas)\n",
    "            data_labels.append(aux_label)\n",
    "\n",
    "        data_mapas = np.array(data_mapas)\n",
    "        data_labels = np.array(data_labels, dtype='int8')\n",
    "        \n",
    "        data_labels_cat = to_categorical(data_labels, num_classes=2)\n",
    "\n",
    "        return data_mapas, data_labels_cat\n",
    "\n",
    "    def get_test_index(data, test_subj_ix):\n",
    "        test_index = False\n",
    "        for subj_i in test_subj_ix:\n",
    "            for subj_j in test_subj_ix:\n",
    "                aux = (data[:,0] == subj_i) | (data[:,1] == subj_j)\n",
    "                test_index |= aux\n",
    "\n",
    "        return test_index\n",
    "\n",
    "    def get_train_index(data, test_subj_ix):\n",
    "        test_index = False\n",
    "        for subj_i in test_subj_ix:\n",
    "            for subj_j in test_subj_ix:\n",
    "                aux = (data[:,0] == subj_i) & (data[:,1] == subj_j)\n",
    "                test_index |= aux\n",
    "\n",
    "        return test_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from my_classes import DataGenerator\n",
    "\n",
    "# Parameters\n",
    "params = {'dim_x': 32,\n",
    "          'dim_y': 32,\n",
    "          'dim_z': 32,\n",
    "          'batch_size': 32,\n",
    "          'shuffle': True}\n",
    "\n",
    "# Datasets\n",
    "partition = # IDs\n",
    "labels = # Labels\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(**params).generate(labels, partition['train'])\n",
    "validation_generator = DataGenerator(**params).generate(labels, partition['validation'])\n",
    "\n",
    "# Design model\n",
    "model = Sequential()\n",
    "[...] # Architecture\n",
    "model.compile()\n",
    "\n",
    "# Train model on dataset\n",
    "model.fit_generator(generator = training_generator,\n",
    "                    steps_per_epoch = len(partition['train'])//batch_size,\n",
    "                    validation_data = validation_generator,\n",
    "                    validation_steps = len(partition['validation'])//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
